{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/unmentioned/UTS_ML2019_ID97114033/blob/master/A1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1nh34yYVfQE",
        "colab_type": "text"
      },
      "source": [
        "# 31005 Machine Learning: Assignment 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6TEusWCVoho",
        "colab_type": "text"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "This report is for Assessment 1 of Machine Learning 31005. For this report, an analysis will conducted on the paper \"Support-Vector Networks\" written by C. Cortes and V.Vapnik, as well as a critique on the paper itself."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEVZRMvJQvol",
        "colab_type": "text"
      },
      "source": [
        "https://github.com/unmentioned/UTS_ML2019_ID97114033/blob/master/A1.ipynb "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WKtlDCuIVo4N",
        "colab_type": "text"
      },
      "source": [
        "# Content\n",
        "The paper, Support-Vector Networks written by C.Cortest and V.Vapnik introduces a new (or improved) machine learning algorithm for binary group classification problems. This algorithm essentially generates a hyperplane (a subspace whose dimension is one less than its ambient space) which separates two classes of items, which can then be used to classify data items in the test set. \n",
        "\n",
        "The idea of Support vector networks was previously introduced by Vapnik in 1962, but had 2 fundamental issues which made SVN’s only viable for use in situations where the training data set could be separated without errors (this meant that the original SVM algorithm could only perform linear classifications without errors). \n",
        "\n",
        "The first issue was finding a hyperplane that could generalize that data well, which was resolved by Vapnik in 1965 whereby taking a small sample of training data (called support vectors), a margin could be determined. This margin allows the hyperplane to be equidistant from the two classifying groups which minimises the hyperplane from misclassifying data.\n",
        "\n",
        "The second problem was treating the high dimensional space, as the algorithm could quite potentially reach an extremely large dimensional feature space, making this algorithm computationally heavy. It was found that a nonlinear classifier could be reached by performing an inner product of the data (via a Kernel function). By doing this, the data can be mapped to a higher dimension and be linearly classified. This solution allows problems that can’t be solved by linear classifiers to be solved.\n",
        "\n",
        "In addition to the technical details for implementing the support vector network, the paper also displays experimental results comparing the performance of the SVN to other algorithms to benchmark problems.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LemfVjxPVpC3",
        "colab_type": "text"
      },
      "source": [
        "# Innovation\n",
        "While the Support Vector Network was introduced in 1962, it was not until 1995 when this paper was released, that an SVN could solve non-linear classification problems. This would then solidify SVN as an independent machine learning algorithm, different from other learning machines at that time. This paper, makes fundamental improvements to the original SVN algorithm, extending its capabilities to include non-linear classification and regression problems. As such, this paper is not merely on optimizing an SVN but introduces a new algorithm under the same methodology. \n",
        "\n",
        "The paper is mainly separated into 5 sections excluding the introduction, which are:\n",
        "1.\tOptimal Hyperplanes\n",
        "2.\tThe Soft Margin Hyperplane\n",
        "3.\tThe Method of Convolution of the Dot-Product in Feature Space\n",
        "4.\tGeneral Features of Support-Vector Networks\n",
        "5.\tExperimental analysis\n",
        "\n",
        "\n",
        "\n",
        "Regarding the innovation of this paper, the first section revisits the methodology of optimising Hyperplanes from previous papers. Which gives context to the algorithm but isn’t new. Sections 2 and 3 introduce new concepts to the SVN algorithm, extending its capabilities. \n",
        "\n",
        "Section 2 introduces the concept of soft margins which allow for classification with a minimal amount of errors when a problem cannot be separated without error. Section 3 introduces the notion of constructing hyperplanes in a feature space from an input vector and input function (or otherwise a kernel function). These functions allow the transformation of data to a higher dimension space where data can be separated making non-linear separable cases solvable.\n",
        "\n",
        "Section 4 shows an in-depth analysis of the parameters that can be tuned which isn’t innovative. The final section shows the experimental method and result of multiple machine learning algorithms on a benchmark problem, showing the effectiveness of SVN’s on two different problems differing in nature. In conclusion, this paper is quite innovative, in that it introduces new features which makes an entire class of problems possible to solve on a pre-existing algorithm.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PCNODNBVpJy",
        "colab_type": "text"
      },
      "source": [
        "# Technical Quality\n",
        "\n",
        "From the experimental analysis section of this paper, the SVN algorithm was experimented on two types of problem. The first was on an artificially generated set on a plane and the second was on real life data on digit recognition.\n",
        "\n",
        "At first glance, the first experiment is of poor technical quality. From the two graphs given, it merely displays the separation of the data items, the support vectors, and errors. It does not give much context to the problem and shows only the transformed data, which can be improved, if an illustration of the data prior to the transformation is shown. However, it can still be used as a reference for how an SVN ‘s results should be structured.\n",
        "\n",
        "The second problem is much more extensively covered. The problem was performed on two different datasets, one for the US postal service database and the other with the NIST database, which allows for variety. Each experiment was benchmarked with other notable machine learning algorithms and included tabulated results which compared the raw error of each machine. It also included tabulated results of the SVN on different parameters. To conclude, apart from the first experiment which was of poor quality, the rest of the experimental results were of high quality as data was sourced from a variety of sources and tested against other machines for comparison.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJloRWZMV8dM",
        "colab_type": "text"
      },
      "source": [
        "# Application and X-Factor\n",
        "\n",
        "SVM’s are commonly used for many classification problems including text categorization, handwriting recognition, face detection, image recognition and more. Interestingly, most of its prominent applications are related to classifying unstructured data types like images or bioorganic information. I believe these applications are well suited due to the properties of SVM’ which are: high dimensional input space, few irrelevant features, sparse documents vectors and text categorization being a fundamentally linearly separable.\n",
        "\n",
        "As the applications of SVM can be quite broad, a few examples which can potentially use SVM’s include music recognition, file classification or any problem that uses unstructured data. \n",
        "\n",
        "From the contents of the paper, it is rather straight forward in its implementation, which personally I don’t think would be good for discussion. Although reading the paper, I found it interesting that despite all the mathematical formulae used to construct an SVM, the idea behind an SVM is rather simple and straightforward as compared to other learning machine algorithms. It is also surprising to see that although SVM are simpler compared to other algorithms, it is also in most cases, more accurate in certain problems.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46x95kTtV-4x",
        "colab_type": "text"
      },
      "source": [
        "# Presentation\n",
        "\n",
        "The structure of this report was done fairly well, with a logical separation of concerns for each subheading. Although I did find it difficult following the paper at times, due to the illustrations. \n",
        "\n",
        "Particularly figure 5, where the graphs elements are elaborated within the article text. It could’ve been more readable if it had a legend of some sort embedded in the illustration. Additionally, sometimes illustrations are placed awkwardly between the middle of a sentence which made it harder to follow especially when the text refers to the diagram before and after the illustration.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRQTbjkgWBYV",
        "colab_type": "text"
      },
      "source": [
        "# References\n",
        "\n",
        "References\n",
        "Cortes, C. and Vapnik, V. (1995). Journal search results - Cite This For Me. Machine Learning, 20(3), pp.273-297.\n",
        "\n",
        "\n",
        "Joachims, T. (1998). Text categorization with Support Vector Machines: Learning with many relevant features. Machine Learning: ECML-98, pp.137-142.\n",
        "\n",
        "\n",
        "Ray, S. (2017). Understanding Support Vector Machines algorithm (along with code). [online] Analytics Vidhya. Available at: https://www.analyticsvidhya.com/blog/2017/09/understaing-support-vector-machine-example-code/ [Accessed 28 Aug. 2019]."
      ]
    }
  ]
}